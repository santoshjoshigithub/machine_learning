{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important concepts around Ensembling.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bias: When a model has high bias, this means that means it doesn't do a good job of bending to the data. An example of an algorithm that usually has high bias is linear regression. Even with completely different datasets, we end up with the same line fit to the data. When models have high bias, this is bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Variance: When a model has high variance, this means that it changes drastically to meet the needs of every point in our dataset. Linear models like the one above has low variance, but high bias. An example of an algorithm that tends to have high variance and low bias is a decision tree (especially decision trees with no early stopping parameters). A decision tree, as a high variance algorithm, will attempt to split every point into its own branch if possible. This is a trait of high variance, low bias algorithms - they are extremely flexible to fit exactly whatever data they see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. By combining algorithms, we can often build models that perform better by meeting in the middle in terms of bias and variance. There are some other tactics that are used to combine algorithms in ways that help them perform better as well. These ideas are based on minimizing bias and variance based on mathematical theories, like the central limit theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Another method that is used to improve ensemble methods is to introduce randomness into high variance algorithms before they are ensembled together. The introduction of randomness combats the tendency of these algorithms to overfit (or fit directly to the data available). There are two main ways that randomness is introduced:\n",
    "\n",
    "    a. Bootstrap the data - that is, sampling the data with replacement and fitting your algorithm to the sampled data.\n",
    "\n",
    "    b. Subset the features - in each split of a decision tree or with each algorithm used in an ensemble, only a subset of the total possible features are used.\n",
    "\n",
    "This is how RANDOM FOREST works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Bagging and Boosting are 2 of the Ensembiling techiniques we widely use. Random forest is one of the example of Bagging. Random Forest is built on top of Decision trees.\n",
    "\n",
    "Bootstrapping the data plus using the aggregate to make a decision is called Bagging.\n",
    "\n",
    "Bagging = Bootstrapping + Aggregating (Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Decision trees are simple, easy to interpret but not very accurate. They do well in the training data but fails miserably in the test or unseen data. To overcome Decsion trees limitation Randome forest is used. RF improves the accuracy of the DT exponentially. In RF first bootstrap data set is created. Then DTs are created using bootstrapped data set but only using a random subset of variables at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Out of Bag Dataset - There are few entries in the source data set that may not make to the boot strapped data set ude to randomization nature of the algorithim and they are called as out of bag data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. In RF there is no predetermined depth of the trees, it's random but in the AdaBoost, the trees are usually a node and 2 children which is called as Stump instead of a tree so in Adaboost it's forest of stumps instead of trees like in RF. Stumps are not great for making classifications.\n",
    "\n",
    "In RF each node has equal vote on the final classifcation whereas it's not the case with Adaboost. Also, in AB order of building stumps is important in weightage of the vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. AB combines a lot of week learners to make classifications. These weak learners are known as Stumps. some stumps get more say in the classification than others. Each stump is made by taking the previous stump's mistake into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Below is the code for AdaBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library..\n",
    "from sklearn.ensemble  import AdaBoostClassifier\n",
    "\n",
    "# init the model..\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "# fit / train the model..\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict the model ..\n",
    "model.predict(X_test)\n",
    "\n",
    "# Trainings after hyper parameter tuning..\n",
    "# base_estimator - The model utilized for the weak learners \n",
    "# (Warning: Don't forget to import the model that you decide to use for#  the weak learner).\n",
    "# n_estimators - The maximum number of weak learners used.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=2), n_estimators = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
