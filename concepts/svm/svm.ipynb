{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important SVM Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SVM can be used for both Classification and Regression types of problems. Diff types of SVM classifiers could be:\n",
    "    \n",
    "    a. Maximum Margin Classifier\n",
    "    \n",
    "    b. Classification with Inseparable Classes\n",
    "    \n",
    "    c. Kernel Methods\n",
    "    \n",
    " Note: there could be more types, need to research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The SVM in particular defines the crite\n",
    "rion to be looking for a decision surface that is maximally far away from any data point. This distance from the decision surface to the closest data point determines the margin of the classifier. And the goal is to maximize this margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. when we allow the misclassification the distance between the observations and the threshold is called as Soft Margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Cross validation can be used to identify the best soft margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The name \"support vector\" comes from the fact that the observations on the edge and within the soft margin are called support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. SVM assumes that your inputs are numeric. If you have categorical inputs you may need to covert them to binary dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Hyper parameters:\n",
    "    \n",
    "    a. The C Parameters\n",
    "    \n",
    "    b. Kernel\n",
    "        i. polynomial (degree of the polynomial parameter)\n",
    "        \n",
    "        ii. rbf (radial basis function) (Gamma Parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. C Parameter:\n",
    "    \n",
    "    a. Error = C * Classification Error + Margin Error\n",
    "    \n",
    "    b. When C is large we are focusing more on classifying our points correctly rather than finding a good margin. If C is small than we are focusing mainly on Margin error.\n",
    "    \n",
    "    c. Small C = Large Margin and may make classification errors; Large C = Small Margin and classify data points more correctly. \n",
    "    \n",
    "    d. Since C is a hyper parameter we need to use GirdSearch to fine tune it's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Kernel Trick: When we can't separate points with a linear line we use the concept of Kernel Trick. It allows us to operate in the original feature space without computing the coordinates of the data in a higher dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. The degree of the polynomial kernel is a Hyper parameter which we can tune using GridSearch technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Gamma Parameter: This is also a hyper parameter we can tune in SVM. Higher the value of Gamma overfit the model will be and smaller the value more underfit the model will be. Large gamma will give you the narrow curve in multi dimensional space and smaller gamma will give your wider curves (mountains)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Below is python code sample for using SVM for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library..\n",
    "from sklearn.svm import SVC\n",
    "# init the model..\n",
    "model = SVC()\n",
    "\n",
    "# fit / train the model..\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict the model ..\n",
    "model.predict(X_test)\n",
    "\n",
    "# Trainings after hyper parameter tuning..\n",
    "\n",
    "# C: The C parameter.\n",
    "# kernel: The kernel. The most common ones are 'linear', 'poly', and 'rbf'.\n",
    "# degree: If the kernel is polynomial, this is the maximum degree of the monomials in the kernel.\n",
    "# gamma : If the kernel is rbf, this is the gamma parameter.\n",
    "\n",
    "model = SVC(kernel='poly', degree=4, C=0.1)\n",
    "model = SVC(kernel='rbf', gamma=27)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
