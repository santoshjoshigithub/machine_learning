{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Decision Trees Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data world entropy is an expression of the disorder, or randomness of a system, or of the lack of information about it. In another words, more the entropy less the knowledge is and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for Entropy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Entropy = - \\sum_{n=1}^{n} p_i \\log_2 (p_i) $$\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Decision Trees can be used for both Regression and Classifcation kind of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The core algorithm for building decision trees called ID3 by J. R. Quinlan which employs a top-down, greedy search through the space of possible branches with no backtracking. ID3 uses Entropy and Information Gain to construct a decision tree. But there are few more versions of underlying decision tree algorithims e.g. C4.5, CART, CHAID, MARS etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Information Gain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Information Gain is basically change in the Entropy. E.g. difference of the entropies between parent node and average of the child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain helps us how to choose a particular feature which will help to split the tree. A tree is split with a node with highest Information Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Hyper parameters of Decision Trees:\n",
    "    \n",
    "        Maximum Depth of a Decision Tree (max_depth)\n",
    "        Minimum number of samples to split (min_samples_split)\n",
    "        Minimum number of samples per leaf (min_samples_leaf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Large depth very often causes overfitting, since a tree that is too deep, can memorize the data. Small depth can result in a very simple model, which may cause underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Small minimum samples per split may result in a complicated, highly branched tree, which can mean the model has memorized the data, or in other words, overfit. Large minimum samples may result in the tree not having enough flexibility to get built, and may result in underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Below is python code sample for using Decision Tree for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library..\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# init the model..\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# fit / train the model..\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict the model ..\n",
    "model.predict(X_test)\n",
    "\n",
    "# Trainings after hyper parameter tuning..\n",
    "model = DecisionTreeClassifier(max_depth = 7, min_samples_leaf = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. In order to determine which separation is best there is a measure to compare known as \"impurity\". There are many ways to measure the impurity but the most popular one is \"gini\". \n",
    "\n",
    "    gini impurity for a particular leaf node = 1 - (prob of yes)2 - (prob of no)2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Gini impurtiy is preferred over Entropy coz computing Gini is easier than computing Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
